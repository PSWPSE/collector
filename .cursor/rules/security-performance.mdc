# Security & Performance Optimization Rules

## 🔐 **SECURITY FIRST PRINCIPLES**

### **Authentication & Authorization**
```python
# JWT Implementation with proper security
import jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext
from fastapi import HTTPException, status

class SecurityManager:
    def __init__(self):
        self.pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
        self.secret_key = os.getenv("JWT_SECRET_KEY")
        self.algorithm = "HS256"
        self.token_expire_minutes = 30
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        return self.pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        return self.pwd_context.hash(password)
    
    def create_access_token(self, data: dict) -> str:
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(minutes=self.token_expire_minutes)
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, token: str) -> dict:
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
```

### **Input Validation & Sanitization**
```python
from pydantic import BaseModel, validator, Field
import re
from urllib.parse import urlparse

class SecureConversionRequest(BaseModel):
    url: str = Field(..., max_length=2048)
    converter_type: str = Field("auto", regex="^(auto|openai|anthropic|local)$")
    user_id: int = Field(..., gt=0)
    
    @validator('url')
    def validate_url(cls, v):
        # Basic URL validation
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL must start with http:// or https://')
        
        # Parse URL to check components
        parsed = urlparse(v)
        if not parsed.netloc:
            raise ValueError('Invalid URL format')
        
        # Block internal/local URLs
        blocked_domains = ['localhost', '127.0.0.1', '0.0.0.0', '10.', '192.168.', '172.16.']
        if any(parsed.netloc.startswith(blocked) for blocked in blocked_domains):
            raise ValueError('Local URLs are not allowed')
        
        return v
    
    @validator('converter_type')
    def validate_converter_type(cls, v):
        allowed_types = ['auto', 'openai', 'anthropic', 'local']
        if v not in allowed_types:
            raise ValueError(f'Converter type must be one of: {allowed_types}')
        return v

# WordPress integration security
def sanitize_wordpress_input(data: str) -> str:
    """Sanitize input for WordPress integration"""
    # Remove potentially dangerous characters
    dangerous_chars = ['<', '>', '"', "'", '&', 'javascript:', 'data:', 'vbscript:']
    for char in dangerous_chars:
        data = data.replace(char, '')
    
    # Limit length
    return data[:1000]
```

### **Rate Limiting & DDoS Protection**
```python
import redis
import time
from functools import wraps
from fastapi import HTTPException, Request

class AdvancedRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def check_rate_limit(self, key: str, limit: int, window: int) -> bool:
        """Sliding window rate limiting"""
        current_time = time.time()
        pipe = self.redis.pipeline()
        
        # Remove old entries
        pipe.zremrangebyscore(key, 0, current_time - window)
        
        # Count current requests
        pipe.zcard(key)
        
        # Add current request
        pipe.zadd(key, {str(current_time): current_time})
        
        # Set expiration
        pipe.expire(key, window)
        
        results = pipe.execute()
        current_requests = results[1]
        
        return current_requests < limit
    
    async def check_progressive_limit(self, user_id: int, user_plan: str) -> bool:
        """Progressive rate limiting based on user plan"""
        limits = {
            'free': {'requests': 5, 'window': 3600},     # 5 per hour
            'premium': {'requests': 1000, 'window': 3600}, # 1000 per hour
            'enterprise': {'requests': 10000, 'window': 3600} # 10000 per hour
        }
        
        limit_config = limits.get(user_plan, limits['free'])
        key = f"rate_limit:{user_id}"
        
        return await self.check_rate_limit(
            key, 
            limit_config['requests'], 
            limit_config['window']
        )

# DDoS protection middleware
@app.middleware("http")
async def ddos_protection(request: Request, call_next):
    client_ip = request.client.host
    
    # Check for suspicious patterns
    if await is_suspicious_ip(client_ip):
        raise HTTPException(status_code=429, detail="Suspicious activity detected")
    
    # Log all requests for monitoring
    await log_request(client_ip, request.url.path, request.method)
    
    response = await call_next(request)
    return response
```

### **API Key Security**
```python
import os
from cryptography.fernet import Fernet
import base64

class APIKeyManager:
    def __init__(self):
        self.encryption_key = self._get_or_create_key()
        self.fernet = Fernet(self.encryption_key)
    
    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key"""
        key = os.getenv("API_ENCRYPTION_KEY")
        if not key:
            key = Fernet.generate_key()
            # In production, store this securely
            os.environ["API_ENCRYPTION_KEY"] = key.decode()
        
        if isinstance(key, str):
            key = key.encode()
        
        return key
    
    def encrypt_api_key(self, api_key: str) -> str:
        """Encrypt API key for storage"""
        encrypted = self.fernet.encrypt(api_key.encode())
        return base64.b64encode(encrypted).decode()
    
    def decrypt_api_key(self, encrypted_key: str) -> str:
        """Decrypt API key for use"""
        encrypted_bytes = base64.b64decode(encrypted_key.encode())
        decrypted = self.fernet.decrypt(encrypted_bytes)
        return decrypted.decode()
    
    def rotate_encryption_key(self) -> None:
        """Rotate encryption key (for security best practices)"""
        # This should be implemented with proper key rotation strategy
        pass
```

## ⚡ **PERFORMANCE OPTIMIZATION**

### **Database Optimization**
```python
from sqlalchemy import create_engine, text, Index
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
import asyncio

class DatabaseOptimizer:
    def __init__(self, database_url: str):
        self.engine = create_async_engine(
            database_url,
            # Connection pooling
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600,
            # Performance tuning
            echo=False,
            future=True
        )
        self.SessionLocal = sessionmaker(
            bind=self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
    
    async def optimize_queries(self):
        """Create indexes for better query performance"""
        async with self.engine.begin() as conn:
            # Usage tracking indexes
            await conn.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_user_usage_date 
                ON user_usage(user_id, date);
                
                CREATE INDEX IF NOT EXISTS idx_conversion_user_time 
                ON conversions(user_id, created_at);
                
                CREATE INDEX IF NOT EXISTS idx_conversion_status 
                ON conversions(status, created_at);
                
                CREATE INDEX IF NOT EXISTS idx_user_plan_type 
                ON users(plan_type);
            """))
    
    async def clean_old_data(self):
        """Clean up old data to maintain performance"""
        async with self.SessionLocal() as session:
            # Delete usage records older than 1 year
            await session.execute(text("""
                DELETE FROM user_usage 
                WHERE date < NOW() - INTERVAL '1 year';
            """))
            
            # Delete failed conversions older than 30 days
            await session.execute(text("""
                DELETE FROM conversions 
                WHERE status = 'failed' 
                AND created_at < NOW() - INTERVAL '30 days';
            """))
            
            await session.commit()
```

### **Caching Strategy**
```python
import redis.asyncio as redis
import json
import hashlib
from typing import Any, Optional
import asyncio

class MultiTierCache:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.memory_cache = {}
        self.memory_cache_ttl = 300  # 5 minutes
    
    def _generate_key(self, prefix: str, data: Any) -> str:
        """Generate cache key from data"""
        data_str = json.dumps(data, sort_keys=True)
        hash_key = hashlib.md5(data_str.encode()).hexdigest()
        return f"{prefix}:{hash_key}"
    
    async def get(self, key: str) -> Optional[Any]:
        """Get from cache (memory first, then Redis)"""
        # Check memory cache first
        if key in self.memory_cache:
            data, timestamp = self.memory_cache[key]
            if time.time() - timestamp < self.memory_cache_ttl:
                return data
            else:
                del self.memory_cache[key]
        
        # Check Redis cache
        cached_data = await self.redis.get(key)
        if cached_data:
            data = json.loads(cached_data)
            # Store in memory cache
            self.memory_cache[key] = (data, time.time())
            return data
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """Set in both memory and Redis cache"""
        # Store in memory
        self.memory_cache[key] = (value, time.time())
        
        # Store in Redis
        await self.redis.setex(key, ttl, json.dumps(value))
    
    async def invalidate_pattern(self, pattern: str) -> None:
        """Invalidate cache entries matching pattern"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
        
        # Clear memory cache entries
        to_delete = [k for k in self.memory_cache.keys() if pattern in k]
        for k in to_delete:
            del self.memory_cache[k]

# Cache decorator
def cache_result(prefix: str, ttl: int = 3600):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = MultiTierCache(redis_url)
            cache_key = cache._generate_key(prefix, {'args': args, 'kwargs': kwargs})
            
            # Try to get from cache
            cached_result = await cache.get(cache_key)
            if cached_result:
                return cached_result
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            await cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### **Async Processing**
```python
import asyncio
from celery import Celery
from typing import List, Dict, Any
import aiohttp
import concurrent.futures

# Celery for background tasks
celery_app = Celery(
    'newsforge',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task
def process_bulk_conversion(urls: List[str], user_id: int) -> Dict[str, Any]:
    """Process multiple URL conversions in background"""
    results = []
    
    for url in urls:
        try:
            # Process each URL
            result = asyncio.run(convert_single_url(url, user_id))
            results.append({"url": url, "success": True, "result": result})
        except Exception as e:
            results.append({"url": url, "success": False, "error": str(e)})
    
    return {"total": len(urls), "results": results}

# Async HTTP client optimization
class OptimizedHTTPClient:
    def __init__(self):
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=30)
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(
            limit=100,  # Total connection limit
            limit_per_host=10,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=self.timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Fetch multiple URLs concurrently"""
        tasks = [self.fetch_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [
            {"url": url, "result": result, "success": not isinstance(result, Exception)}
            for url, result in zip(urls, results)
        ]
    
    async def fetch_single(self, url: str) -> str:
        """Fetch single URL with retry logic"""
        for attempt in range(3):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        raise aiohttp.ClientError(f"HTTP {response.status}")
            except Exception as e:
                if attempt == 2:  # Last attempt
                    raise
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

### **Resource Management**
```python
import psutil
import asyncio
import logging
from typing import Dict, Any

class ResourceMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.thresholds = {
            'cpu_percent': 80,
            'memory_percent': 85,
            'disk_percent': 90
        }
    
    async def monitor_resources(self) -> Dict[str, Any]:
        """Monitor system resources"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'disk_percent': disk.percent,
            'memory_available': memory.available,
            'timestamp': time.time()
        }
        
        # Check thresholds
        alerts = []
        for metric, value in metrics.items():
            if metric.endswith('_percent') and value > self.thresholds.get(metric, 100):
                alerts.append(f"{metric}: {value}%")
        
        if alerts:
            self.logger.warning(f"Resource usage alerts: {', '.join(alerts)}")
        
        return metrics
    
    async def optimize_if_needed(self, metrics: Dict[str, Any]) -> None:
        """Optimize resources if thresholds are exceeded"""
        if metrics['memory_percent'] > self.thresholds['memory_percent']:
            # Clear memory caches
            await self.clear_memory_caches()
        
        if metrics['cpu_percent'] > self.thresholds['cpu_percent']:
            # Reduce concurrent processing
            await self.reduce_concurrency()
    
    async def clear_memory_caches(self) -> None:
        """Clear memory caches to free up memory"""
        # Clear application caches
        if hasattr(self, 'cache_manager'):
            await self.cache_manager.clear_memory_cache()
        
        # Force garbage collection
        import gc
        gc.collect()
    
    async def reduce_concurrency(self) -> None:
        """Reduce concurrent processing to lower CPU usage"""
        # Implement dynamic concurrency control
        pass
```

## 🛡️ **SECURITY MONITORING**

### **Security Event Logging**
```python
import logging
import json
from datetime import datetime
from typing import Dict, Any

class SecurityLogger:
    def __init__(self):
        self.logger = logging.getLogger('security')
        self.logger.setLevel(logging.WARNING)
        
        # Create file handler for security logs
        handler = logging.FileHandler('security.log')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_security_event(self, event_type: str, details: Dict[str, Any]) -> None:
        """Log security events"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'details': details
        }
        
        self.logger.warning(json.dumps(log_entry))
    
    def log_failed_auth(self, ip: str, user_agent: str, attempted_user: str) -> None:
        """Log failed authentication attempts"""
        self.log_security_event('failed_auth', {
            'ip': ip,
            'user_agent': user_agent,
            'attempted_user': attempted_user
        })
    
    def log_rate_limit_exceeded(self, ip: str, user_id: int, endpoint: str) -> None:
        """Log rate limit violations"""
        self.log_security_event('rate_limit_exceeded', {
            'ip': ip,
            'user_id': user_id,
            'endpoint': endpoint
        })
    
    def log_suspicious_activity(self, ip: str, activity: str, details: Dict[str, Any]) -> None:
        """Log suspicious activities"""
        self.log_security_event('suspicious_activity', {
            'ip': ip,
            'activity': activity,
            'details': details
        })

# Security middleware
@app.middleware("http")
async def security_monitoring(request: Request, call_next):
    start_time = time.time()
    client_ip = request.client.host
    
    # Log all API calls
    security_logger = SecurityLogger()
    
    try:
        response = await call_next(request)
        
        # Log slow requests (potential DoS attempts)
        processing_time = time.time() - start_time
        if processing_time > 10:  # 10 seconds
            security_logger.log_suspicious_activity(
                client_ip,
                'slow_request',
                {
                    'processing_time': processing_time,
                    'endpoint': request.url.path,
                    'method': request.method
                }
            )
        
        return response
        
    except Exception as e:
        # Log all errors for security analysis
        security_logger.log_security_event('request_error', {
            'ip': client_ip,
            'endpoint': request.url.path,
            'error': str(e)
        })
        raise
```

**Remember**: Security is not optional—it's the foundation. Every feature must be secure by design. Performance optimization should never compromise security. Monitor everything, log everything, and assume breach scenarios in your architecture.
description:
globs:
alwaysApply: false
---
