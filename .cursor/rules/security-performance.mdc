# Security & Performance Optimization Rules

## ðŸ” **SECURITY FIRST PRINCIPLES**

### **Authentication & Authorization**
```python
# JWT Implementation with proper security
import jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext
from fastapi import HTTPException, status

class SecurityManager:
    def __init__(self):
        self.pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
        self.secret_key = os.getenv("JWT_SECRET_KEY")
        self.algorithm = "HS256"
        self.token_expire_minutes = 30
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        return self.pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        return self.pwd_context.hash(password)
    
    def create_access_token(self, data: dict) -> str:
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(minutes=self.token_expire_minutes)
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, token: str) -> dict:
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
```

### **Input Validation & Sanitization**
```python
from pydantic import BaseModel, validator, Field
import re
from urllib.parse import urlparse

class SecureConversionRequest(BaseModel):
    url: str = Field(..., max_length=2048)
    converter_type: str = Field("auto", regex="^(auto|openai|anthropic|local)$")
    user_id: int = Field(..., gt=0)
    
    @validator('url')
    def validate_url(cls, v):
        # Basic URL validation
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL must start with http:// or https://')
        
        # Parse URL to check components
        parsed = urlparse(v)
        if not parsed.netloc:
            raise ValueError('Invalid URL format')
        
        # Block internal/local URLs
        blocked_domains = ['localhost', '127.0.0.1', '0.0.0.0', '10.', '192.168.', '172.16.']
        if any(parsed.netloc.startswith(blocked) for blocked in blocked_domains):
            raise ValueError('Local URLs are not allowed')
        
        return v
    
    @validator('converter_type')
    def validate_converter_type(cls, v):
        allowed_types = ['auto', 'openai', 'anthropic', 'local']
        if v not in allowed_types:
            raise ValueError(f'Converter type must be one of: {allowed_types}')
        return v

# WordPress integration security
def sanitize_wordpress_input(data: str) -> str:
    """Sanitize input for WordPress integration"""
    # Remove potentially dangerous characters
    dangerous_chars = ['<', '>', '"', "'", '&', 'javascript:', 'data:', 'vbscript:']
    for char in dangerous_chars:
        data = data.replace(char, '')
    
    # Limit length
    return data[:1000]
```

### **Rate Limiting & DDoS Protection**
```python
import redis
import time
from functools import wraps
from fastapi import HTTPException, Request

class AdvancedRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def check_rate_limit(self, key: str, limit: int, window: int) -> bool:
        """Sliding window rate limiting"""
        current_time = time.time()
        pipe = self.redis.pipeline()
        
        # Remove old entries
        pipe.zremrangebyscore(key, 0, current_time - window)
        
        # Count current requests
        pipe.zcard(key)
        
        # Add current request
        pipe.zadd(key, {str(current_time): current_time})
        
        # Set expiration
        pipe.expire(key, window)
        
        results = pipe.execute()
        current_requests = results[1]
        
        return current_requests < limit
    
    async def check_progressive_limit(self, user_id: int, user_plan: str) -> bool:
        """Progressive rate limiting based on user plan"""
        limits = {
            'free': {'requests': 5, 'window': 3600},     # 5 per hour
            'premium': {'requests': 1000, 'window': 3600}, # 1000 per hour
            'enterprise': {'requests': 10000, 'window': 3600} # 10000 per hour
        }
        
        limit_config = limits.get(user_plan, limits['free'])
        key = f"rate_limit:{user_id}"
        
        return await self.check_rate_limit(
            key, 
            limit_config['requests'], 
            limit_config['window']
        )

# DDoS protection middleware
@app.middleware("http")
async def ddos_protection(request: Request, call_next):
    client_ip = request.client.host
    
    # Check for suspicious patterns
    if await is_suspicious_ip(client_ip):
        raise HTTPException(status_code=429, detail="Suspicious activity detected")
    
    # Log all requests for monitoring
    await log_request(client_ip, request.url.path, request.method)
    
    response = await call_next(request)
    return response
```

### **API Key Security**
```python
import os
from cryptography.fernet import Fernet
import base64

class APIKeyManager:
    def __init__(self):
        self.encryption_key = self._get_or_create_key()
        self.fernet = Fernet(self.encryption_key)
    
    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key"""
        key = os.getenv("API_ENCRYPTION_KEY")
        if not key:
            key = Fernet.generate_key()
            # In production, store this securely
            os.environ["API_ENCRYPTION_KEY"] = key.decode()
        
        if isinstance(key, str):
            key = key.encode()
        
        return key
    
    def encrypt_api_key(self, api_key: str) -> str:
        """Encrypt API key for storage"""
        encrypted = self.fernet.encrypt(api_key.encode())
        return base64.b64encode(encrypted).decode()
    
    def decrypt_api_key(self, encrypted_key: str) -> str:
        """Decrypt API key for use"""
        encrypted_bytes = base64.b64decode(encrypted_key.encode())
        decrypted = self.fernet.decrypt(encrypted_bytes)
        return decrypted.decode()
    
    def rotate_encryption_key(self) -> None:
        """Rotate encryption key (for security best practices)"""
        # This should be implemented with proper key rotation strategy
        pass
```

## âš¡ **PERFORMANCE OPTIMIZATION**

### **Database Optimization**
```python
from sqlalchemy import create_engine, text, Index
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
import asyncio

class DatabaseOptimizer:
    def __init__(self, database_url: str):
        self.engine = create_async_engine(
            database_url,
            # Connection pooling
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600,
            # Performance tuning
            echo=False,
            future=True
        )
        self.SessionLocal = sessionmaker(
            bind=self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
    
    async def optimize_queries(self):
        """Create indexes for better query performance"""
        async with self.engine.begin() as conn:
            # Usage tracking indexes
            await conn.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_user_usage_date 
                ON user_usage(user_id, date);
                
                CREATE INDEX IF NOT EXISTS idx_conversion_user_time 
                ON conversions(user_id, created_at);
                
                CREATE INDEX IF NOT EXISTS idx_conversion_status 
                ON conversions(status, created_at);
                
                CREATE INDEX IF NOT EXISTS idx_user_plan_type 
                ON users(plan_type);
            """))
    
    async def clean_old_data(self):
        """Clean up old data to maintain performance"""
        async with self.SessionLocal() as session:
            # Delete usage records older than 1 year
            await session.execute(text("""
                DELETE FROM user_usage 
                WHERE date < NOW() - INTERVAL '1 year';
            """))
            
            # Delete failed conversions older than 30 days
            await session.execute(text("""
                DELETE FROM conversions 
                WHERE status = 'failed' 
                AND created_at < NOW() - INTERVAL '30 days';
            """))
            
            await session.commit()
```

### **Caching Strategy**
```python
import redis.asyncio as redis
import json
import hashlib
from typing import Any, Optional
import asyncio

class MultiTierCache:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.memory_cache = {}
        self.memory_cache_ttl = 300  # 5 minutes
    
    def _generate_key(self, prefix: str, data: Any) -> str:
        """Generate cache key from data"""
        data_str = json.dumps(data, sort_keys=True)
        hash_key = hashlib.md5(data_str.encode()).hexdigest()
        return f"{prefix}:{hash_key}"
    
    async def get(self, key: str) -> Optional[Any]:
        """Get from cache (memory first, then Redis)"""
        # Check memory cache first
        if key in self.memory_cache:
            data, timestamp = self.memory_cache[key]
            if time.time() - timestamp < self.memory_cache_ttl:
                return data
            else:
                del self.memory_cache[key]
        
        # Check Redis cache
        cached_data = await self.redis.get(key)
        if cached_data:
            data = json.loads(cached_data)
            # Store in memory cache
            self.memory_cache[key] = (data, time.time())
            return data
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """Set in both memory and Redis cache"""
        # Store in memory
        self.memory_cache[key] = (value, time.time())
        
        # Store in Redis
        await self.redis.setex(key, ttl, json.dumps(value))
    
    async def invalidate_pattern(self, pattern: str) -> None:
        """Invalidate cache entries matching pattern"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
        
        # Clear memory cache entries
        to_delete = [k for k in self.memory_cache.keys() if pattern in k]
        for k in to_delete:
            del self.memory_cache[k]

# Cache decorator
def cache_result(prefix: str, ttl: int = 3600):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = MultiTierCache(redis_url)
            cache_key = cache._generate_key(prefix, {'args': args, 'kwargs': kwargs})
            
            # Try to get from cache
            cached_result = await cache.get(cache_key)
            if cached_result:
                return cached_result
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            await cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### **Async Processing**
```python
import asyncio
from celery import Celery
from typing import List, Dict, Any
import aiohttp
import concurrent.futures

# Celery for background tasks
celery_app = Celery(
    'newsforge',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task
def process_bulk_conversion(urls: List[str], user_id: int) -> Dict[str, Any]:
    """Process multiple URL conversions in background"""
    results = []
    
    for url in urls:
        try:
            # Process each URL
            result = asyncio.run(convert_single_url(url, user_id))
            results.append({"url": url, "success": True, "result": result})
        except Exception as e:
            results.append({"url": url, "success": False, "error": str(e)})
    
    return {"total": len(urls), "results": results}

# Async HTTP client optimization
class OptimizedHTTPClient:
    def __init__(self):
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=30)
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(
            limit=100,  # Total connection limit
            limit_per_host=10,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=self.timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Fetch multiple URLs concurrently"""
        tasks = [self.fetch_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [
            {"url": url, "result": result, "success": not isinstance(result, Exception)}
            for url, result in zip(urls, results)
        ]
    
    async def fetch_single(self, url: str) -> str:
        """Fetch single URL with retry logic"""
        for attempt in range(3):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        raise aiohttp.ClientError(f"HTTP {response.status}")
            except Exception as e:
                if attempt == 2:  # Last attempt
                    raise
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

### **Resource Management**
```python
import psutil
import asyncio
import logging
from typing import Dict, Any

class ResourceMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.thresholds = {
            'cpu_percent': 80,
            'memory_percent': 85,
            'disk_percent': 90
        }
    
    async def monitor_resources(self) -> Dict[str, Any]:
        """Monitor system resources"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'disk_percent': disk.percent,
            'memory_available': memory.available,
            'timestamp': time.time()
        }
        
        # Check thresholds
        alerts = []
        for metric, value in metrics.items():
            if metric.endswith('_percent') and value > self.thresholds.get(metric, 100):
                alerts.append(f"{metric}: {value}%")
        
        if alerts:
            self.logger.warning(f"Resource usage alerts: {', '.join(alerts)}")
        
        return metrics
    
    async def optimize_if_needed(self, metrics: Dict[str, Any]) -> None:
        """Optimize resources if thresholds are exceeded"""
        if metrics['memory_percent'] > self.thresholds['memory_percent']:
            # Clear memory caches
            await self.clear_memory_caches()
        
        if metrics['cpu_percent'] > self.thresholds['cpu_percent']:
            # Reduce concurrent processing
            await self.reduce_concurrency()
    
    async def clear_memory_caches(self) -> None:
        """Clear memory caches to free up memory"""
        # Clear application caches
        if hasattr(self, 'cache_manager'):
            await self.cache_manager.clear_memory_cache()
        
        # Force garbage collection
        import gc
        gc.collect()
    
    async def reduce_concurrency(self) -> None:
        """Reduce concurrent processing to lower CPU usage"""
        # Implement dynamic concurrency control
        pass
```

## ðŸ›¡ï¸ **SECURITY MONITORING**

### **Security Event Logging**
```python
import logging
import json
from datetime import datetime
from typing import Dict, Any

class SecurityLogger:
    def __init__(self):
        self.logger = logging.getLogger('security')
        self.logger.setLevel(logging.WARNING)
        
        # Create file handler for security logs
        handler = logging.FileHandler('security.log')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_security_event(self, event_type: str, details: Dict[str, Any]) -> None:
        """Log security events"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'details': details
        }
        
        self.logger.warning(json.dumps(log_entry))
    
    def log_failed_auth(self, ip: str, user_agent: str, attempted_user: str) -> None:
        """Log failed authentication attempts"""
        self.log_security_event('failed_auth', {
            'ip': ip,
            'user_agent': user_agent,
            'attempted_user': attempted_user
        })
    
    def log_rate_limit_exceeded(self, ip: str, user_id: int, endpoint: str) -> None:
        """Log rate limit violations"""
        self.log_security_event('rate_limit_exceeded', {
            'ip': ip,
            'user_id': user_id,
            'endpoint': endpoint
        })
    
    def log_suspicious_activity(self, ip: str, activity: str, details: Dict[str, Any]) -> None:
        """Log suspicious activities"""
        self.log_security_event('suspicious_activity', {
            'ip': ip,
            'activity': activity,
            'details': details
        })

# Security middleware
@app.middleware("http")
async def security_monitoring(request: Request, call_next):
    start_time = time.time()
    client_ip = request.client.host
    
    # Log all API calls
    security_logger = SecurityLogger()
    
    try:
        response = await call_next(request)
        
        # Log slow requests (potential DoS attempts)
        processing_time = time.time() - start_time
        if processing_time > 10:  # 10 seconds
            security_logger.log_suspicious_activity(
                client_ip,
                'slow_request',
                {
                    'processing_time': processing_time,
                    'endpoint': request.url.path,
                    'method': request.method
                }
            )
        
        return response
        
    except Exception as e:
        # Log all errors for security analysis
        security_logger.log_security_event('request_error', {
            'ip': client_ip,
            'endpoint': request.url.path,
            'error': str(e)
        })
        raise
```

**Remember**: Security is not optionalâ€”it's the foundation. Every feature must be secure by design. Performance optimization should never compromise security. Monitor everything, log everything, and assume breach scenarios in your architecture.
description:
globs:
alwaysApply: false
---
