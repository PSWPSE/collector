import os
import re
from datetime import datetime
from pathlib import Path
from collections import Counter
import requests
import json
from typing import Dict, List, Optional

class EnhancedNewsConverter:
    def __init__(self, use_online_translation=True):
        """
        ê°œì„ ëœ ë‰´ìŠ¤ ë³€í™˜ê¸°
        
        Args:
            use_online_translation: ì˜¨ë¼ì¸ ë²ˆì—­ ì‚¬ìš© ì—¬ë¶€ (Falseì‹œ ì™„ì „ ì˜¤í”„ë¼ì¸)
        """
        self.use_online_translation = use_online_translation
        self.output_dir = Path('converted_articles')
        self.output_dir.mkdir(exist_ok=True)
        
        # ì´ëª¨ì§€ ë§¤í•‘ (í† í”½ë³„)
        self.emoji_mapping = {
            'market': 'ğŸ“ˆ',
            'finance': 'ğŸ’°',
            'tech': 'ğŸš€',
            'policy': 'âš–ï¸',
            'trade': 'ğŸ¤',
            'conflict': 'ğŸ”¥',
            'growth': 'ğŸŒ±',
            'default': 'ğŸ“°'
        }
        
        # í† í”½ í‚¤ì›Œë“œ (ì˜í•œ ë§¤í•‘)
        self.topic_keywords = {
            'market': {
                'en': ['market', 'stock', 'trading', 'nasdaq', 'dow', 'index', 'equity'],
                'ko': ['ì‹œì¥', 'ì£¼ì‹', 'ê±°ë˜', 'ì§€ìˆ˜']
            },
            'finance': {
                'en': ['bank', 'finance', 'investment', 'fund', 'money', 'dollar'],
                'ko': ['ì€í–‰', 'ê¸ˆìœµ', 'íˆ¬ì', 'í€ë“œ', 'ë‹¬ëŸ¬']
            },
            'tech': {
                'en': ['technology', 'tech', 'ai', 'digital', 'software'],
                'ko': ['ê¸°ìˆ ', 'ë””ì§€í„¸', 'ì†Œí”„íŠ¸ì›¨ì–´']
            },
            'policy': {
                'en': ['policy', 'regulation', 'law', 'government', 'bill'],
                'ko': ['ì •ì±…', 'ê·œì œ', 'ë²•ë¥ ', 'ì •ë¶€']
            },
            'trade': {
                'en': ['trade', 'tariff', 'export', 'import', 'deal'],
                'ko': ['ë¬´ì—­', 'ê´€ì„¸', 'ìˆ˜ì¶œ', 'ìˆ˜ì…', 'í˜‘ìƒ']
            }
        }
        
        # ê¸°ë³¸ ë²ˆì—­ ì‚¬ì „ (ìì£¼ ì‚¬ìš©ë˜ëŠ” ê¸ˆìœµ/ê²½ì œ ìš©ì–´)
        self.translation_dict = {
            # ì‹œì¥ ê´€ë ¨
            'Asian stocks': 'ì•„ì‹œì•„ ì£¼ì‹',
            'stock market': 'ì£¼ì‹ ì‹œì¥',
            'equity market': 'ì£¼ì‹ ì‹œì¥',
            'Wall Street': 'ì›”ê°€',
            'market': 'ì‹œì¥',
            'trading': 'ê±°ë˜',
            'investors': 'íˆ¬ììë“¤',
            'shares': 'ì£¼ì‹',
            
            # ì§€ìˆ˜ ê´€ë ¨
            'Nikkei': 'ë‹ˆì¼€ì´',
            'Hang Seng': 'í•­ì…',
            'KOSPI': 'ì½”ìŠ¤í”¼',
            'Dow': 'ë‹¤ìš°',
            'S&P': 'S&P',
            'NASDAQ': 'ë‚˜ìŠ¤ë‹¥',
            
            # ê²½ì œ ì§€í‘œ
            'GDP': 'GDP',
            'inflation': 'ì¸í”Œë ˆì´ì…˜',
            'interest rate': 'ê¸ˆë¦¬',
            'Federal Reserve': 'ì—°ë°©ì¤€ë¹„ì œë„',
            'jobs report': 'ê³ ìš© ë³´ê³ ì„œ',
            'unemployment': 'ì‹¤ì—…ë¥ ',
            
            # í†µí™”
            'dollar': 'ë‹¬ëŸ¬',
            'yen': 'ì—”',
            'euro': 'ìœ ë¡œ',
            'yuan': 'ìœ„ì•ˆ',
            'won': 'ì›',
            
            # ì •ì±…/ì •ì¹˜
            'President': 'ëŒ€í†µë ¹',
            'government': 'ì •ë¶€',
            'policy': 'ì •ì±…',
            'regulation': 'ê·œì œ',
            'tariff': 'ê´€ì„¸',
            'trade deal': 'ë¬´ì—­ í˜‘ì •',
            
            # ê¸°ì—…/ì‚°ì—…
            'technology': 'ê¸°ìˆ ',
            'artificial intelligence': 'ì¸ê³µì§€ëŠ¥',
            'AI': 'AI',
            'startup': 'ìŠ¤íƒ€íŠ¸ì—…',
            'IPO': 'IPO',
            
            # ë°©í–¥ì„±
            'rose': 'ìƒìŠ¹',
            'fell': 'í•˜ë½',
            'gained': 'ìƒìŠ¹',
            'declined': 'í•˜ë½',
            'increased': 'ì¦ê°€',
            'decreased': 'ê°ì†Œ',
            'up': 'ìƒìŠ¹',
            'down': 'í•˜ë½',
            'higher': 'ìƒìŠ¹',
            'lower': 'í•˜ë½'
        }
        
        # ì„¹ì…˜ í…œí”Œë¦¿
        self.section_templates = {
            'market_status': 'â–¶ ì‹œì¥ í˜„í™©:',
            'economic_data': 'â–¶ ê²½ì œ ì§€í‘œ:',
            'policy_news': 'â–¶ ì •ì±… ë™í–¥:',
            'trade_news': 'â–¶ ë¬´ì—­ í˜‘ìƒ:',
            'company_news': 'â–¶ ê¸°ì—… ë™í–¥:',
            'outlook': 'â–¶ í–¥í›„ ì „ë§:'
        }

    def detect_topic(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í† í”½ ê°ì§€"""
        text_lower = text.lower()
        scores = {topic: 0 for topic in self.topic_keywords.keys()}
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords['en']:
                scores[topic] += text_lower.count(keyword)
        
        if not any(scores.values()):
            return 'default'
        
        return max(scores.items(), key=lambda x: x[1])[0]

    def translate_text_local(self, text: str) -> str:
        """ë¡œì»¬ ë²ˆì—­ ì‚¬ì „ì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë²ˆì—­"""
        translated = text
        
        # ê¸°ë³¸ ë²ˆì—­ ì‚¬ì „ ì ìš©
        for en_term, ko_term in self.translation_dict.items():
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì¹˜í™˜
            translated = re.sub(
                re.escape(en_term), 
                ko_term, 
                translated, 
                flags=re.IGNORECASE
            )
        
        return translated

    def translate_text_online(self, text: str) -> str:
        """ì˜¨ë¼ì¸ ë²ˆì—­ ì„œë¹„ìŠ¤ ì‚¬ìš© (Google Translate ë¬´ë£Œ API)"""
        if not self.use_online_translation:
            return self.translate_text_local(text)
        
        try:
            # Google Translate ë¬´ë£Œ API ì‚¬ìš©
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'en',
                'tl': 'ko',
                'dt': 't',
                'q': text
            }
            
            response = requests.get(url, params=params, timeout=5)
            if response.status_code == 200:
                result = response.json()
                translated = ''.join([item[0] for item in result[0] if item[0]])
                return translated
            else:
                # ì˜¨ë¼ì¸ ë²ˆì—­ ì‹¤íŒ¨ì‹œ ë¡œì»¬ ë²ˆì—­ìœ¼ë¡œ í´ë°±
                return self.translate_text_local(text)
                
        except Exception as e:
            print(f"ì˜¨ë¼ì¸ ë²ˆì—­ ì‹¤íŒ¨, ë¡œì»¬ ë²ˆì—­ ì‚¬ìš©: {e}")
            return self.translate_text_local(text)

    def clean_content(self, content: str) -> str:
        """ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°"""
        # ê¸°ì ì •ë³´ ì œê±°
        content = re.sub(r'By\s+[\w\s]+\n', '', content)
        content = re.sub(r'Kevin Buckland.*?\n', '', content)
        content = re.sub(r'\w+@\w+\.\w+', '', content)  # ì´ë©”ì¼ ì œê±°
        
        # ì¤‘ë³µ ë¼ì¸ ì œê±°
        lines = content.split('\n')
        unique_lines = []
        seen = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen and len(line) > 10:
                unique_lines.append(line)
                seen.add(line)
        
        # ë©”íƒ€ë°ì´í„° ì œê±°
        filtered_lines = []
        for line in unique_lines:
            if not any(skip in line.lower() for skip in [
                'usd=x', '^spx', 'terms and privacy', 'privacy dashboard',
                'fri,', 'min read', 'in this article'
            ]):
                filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)

    def categorize_sentences(self, sentences: List[str]) -> Dict[str, List[str]]:
        """ë¬¸ì¥ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        categories = {
            'market_status': [],
            'economic_data': [],
            'policy_news': [],
            'trade_news': [],
            'company_news': [],
            'outlook': []
        }
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # ì‹œì¥ í˜„í™©
            if any(keyword in sentence_lower for keyword in [
                'market', 'stock', 'index', 'trading', 'nikkei', 'kospi', 'hang seng'
            ]):
                categories['market_status'].append(sentence)
            
            # ê²½ì œ ì§€í‘œ
            elif any(keyword in sentence_lower for keyword in [
                'jobs', 'employment', 'gdp', 'inflation', 'data', 'report'
            ]):
                categories['economic_data'].append(sentence)
            
            # ì •ì±… ë‰´ìŠ¤
            elif any(keyword in sentence_lower for keyword in [
                'president', 'government', 'policy', 'regulation', 'bill'
            ]):
                categories['policy_news'].append(sentence)
            
            # ë¬´ì—­ ë‰´ìŠ¤
            elif any(keyword in sentence_lower for keyword in [
                'trade', 'tariff', 'deal', 'agreement', 'negotiation'
            ]):
                categories['trade_news'].append(sentence)
            
            # í–¥í›„ ì „ë§
            elif any(keyword in sentence_lower for keyword in [
                'expect', 'forecast', 'outlook', 'future', 'ahead'
            ]):
                categories['outlook'].append(sentence)
            
            # ê¸°íƒ€ëŠ” íšŒì‚¬ ë‰´ìŠ¤ë¡œ
            else:
                categories['company_news'].append(sentence)
        
        return categories

    def generate_title(self, content: str, topic: str) -> str:
        """ë‚´ìš© ê¸°ë°˜ ì œëª© ìƒì„±"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        key_phrases = []
        
        if 'asian' in content.lower() and 'stock' in content.lower():
            key_phrases.append('ì•„ì‹œì•„ ì£¼ì‹')
        
        if 'trade' in content.lower() and 'deal' in content.lower():
            key_phrases.append('ë¬´ì—­ í˜‘ìƒ')
        
        if 'tariff' in content.lower():
            key_phrases.append('ê´€ì„¸')
        
        if 'trump' in content.lower():
            key_phrases.append('íŠ¸ëŸ¼í”„')
        
        # ì œëª© êµ¬ì„±
        emoji = self.emoji_mapping.get(topic, self.emoji_mapping['default'])
        
        if key_phrases:
            title = f"{emoji} {', '.join(key_phrases[:2])} ê´€ë ¨ ë™í–¥"
        else:
            title = f"{emoji} ì‹œì¥ ë™í–¥ ë¶„ì„"
        
        return title

    def extract_keywords_smart(self, content: str) -> str:
        """ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'\b[a-zA-Z]{3,}\b', content.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'man', 'men', 'put', 'say', 'she', 'too', 'use'}
        words = [w for w in words if w not in stopwords and len(w) > 3]
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì„ ì •
        word_freq = Counter(words)
        top_words = [word for word, _ in word_freq.most_common(5)]
        
        # í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ í‚¤ì›Œë“œ ì¶”ê°€
        korean_keywords = []
        for word in top_words:
            if word in self.translation_dict:
                korean_keywords.append(self.translation_dict[word])
            else:
                korean_keywords.append(word)
        
        # í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        hashtags = [f"#{keyword.replace(' ', '')}" for keyword in korean_keywords]
        return ' '.join(hashtags)

    def read_txt_file(self, file_path: str) -> Dict[str, str]:
        """TXT íŒŒì¼ ì½ê¸°"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # íŒŒì‹±
        title_match = re.search(r'ì œëª©: (.*?)\n={2,}', content)
        meta_match = re.search(r'ë©”íƒ€ ì •ë³´:\ndescription: (.*?)\n-{2,}', content, re.DOTALL)
        content_match = re.search(r'ë³¸ë¬¸:\n(.*?)$', content, re.DOTALL)
        
        title = title_match.group(1) if title_match else ""
        description = meta_match.group(1).strip() if meta_match else ""
        main_content = content_match.group(1).strip() if content_match else ""
        
        return {
            'title': title,
            'description': description,
            'content': main_content
        }

    def convert_to_markdown(self, data: Dict[str, str]) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³€í™˜"""
        full_content = f"{data['title']} {data['description']} {data['content']}"
        
        # 1. í† í”½ ê°ì§€
        topic = self.detect_topic(full_content)
        
        # 2. ì œëª© ìƒì„±
        title = self.generate_title(full_content, topic)
        
        # 3. ë³¸ë¬¸ ì •ë¦¬
        cleaned_content = self.clean_content(data['content'])
        
        # 4. ë¬¸ì¥ ë¶„ë¦¬ ë° ë²ˆì—­
        sentences = [s.strip() for s in cleaned_content.split('.') if s.strip() and len(s.strip()) > 20]
        
        # 5. ë¬¸ì¥ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        categorized = self.categorize_sentences(sentences)
        
        # 6. ë§ˆí¬ë‹¤ìš´ êµ¬ì„±
        markdown_parts = [title, ""]
        
        for category, template in self.section_templates.items():
            if categorized[category]:
                markdown_parts.append(template)
                for sentence in categorized[category][:3]:  # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ 3ê°œ ë¬¸ì¥
                    # ë²ˆì—­ ì ìš©
                    if self.use_online_translation:
                        translated = self.translate_text_online(sentence)
                    else:
                        translated = self.translate_text_local(sentence)
                    markdown_parts.append(f"â€¢ {translated}")
                markdown_parts.append("")
        
        # 7. í‚¤ì›Œë“œ ì¶”ê°€
        keywords = self.extract_keywords_smart(full_content)
        markdown_parts.extend(["", keywords])
        
        return '\n'.join(markdown_parts)

    def process_file(self, file_path: str) -> None:
        """íŒŒì¼ ì²˜ë¦¬"""
        print(f"Processing {file_path} with Enhanced Converter...")
        
        # íŒŒì¼ ì½ê¸°
        data = self.read_txt_file(file_path)
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        markdown_content = self.convert_to_markdown(data)
        
        # ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f"{Path(file_path).stem}_enhanced_{timestamp}.md"
        output_path = self.output_dir / output_filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"Created {output_path}")

def main():
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python enhanced_converter.py <txt_file> [--offline]")
        print("  --offline: ì™„ì „ ì˜¤í”„ë¼ì¸ ëª¨ë“œ (ì˜¨ë¼ì¸ ë²ˆì—­ ì‚¬ìš© ì•ˆí•¨)")
        sys.exit(1)
    
    file_path = sys.argv[1]
    offline_mode = '--offline' in sys.argv
    
    converter = EnhancedNewsConverter(use_online_translation=not offline_mode)
    
    if os.path.isfile(file_path):
        converter.process_file(file_path)
    else:
        print(f"Error: {file_path} is not a valid file")
        sys.exit(1)

if __name__ == '__main__':
    main() 