"""
ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë‰´ìŠ¤ ë³€í™˜ê¸°

API ì—†ì´ ë¡œì»¬ì—ì„œ ì‘ë™í•˜ëŠ” ë³€í™˜ê¸°ë¡œ, ê·œì¹™ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ 
ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import re
import math
from typing import Dict, List, Tuple
from collections import Counter
from .base_converter import BaseConverter


class LocalConverter(BaseConverter):
    """ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë³€í™˜ê¸°"""
    
    def __init__(self, output_dir: str = 'data/generated'):
        """
        ë¡œì»¬ ë³€í™˜ê¸° ì´ˆê¸°í™”
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        super().__init__(output_dir)
        print("âš¡ Using Local Rule-based Converter")
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ (ì¡°ì‚¬, ì–´ë¯¸ ë“±)
        self.stopwords = {
            'ì´', 'ëŠ”', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'ìœ¼ë¡œë¶€í„°',
            'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'í•œë‹¤', 'í–ˆë‹¤', 'í• ', 'ê²ƒ', 'ìˆ˜', 'ë°', 'ê·¸ë¦¬ê³ ',
            'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ì´ì—', 'ì´ì™€', 'ê°™ì´', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´',
            'ê´€ë ¨', 'ëŒ€í•œ', 'ëŒ€í•´', 'ìœ„í•œ', 'ì•„ë˜', 'ìœ„', 'ì¤‘', 'ë‚´', 'ì™¸', 'ê°„', 'ì „', 'í›„', 'ë™ì•ˆ',
            'ê·¸', 'ì´', 'ì €', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ',
            'ë”', 'ê°€ì¥', 'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì¡°ê¸ˆ', 'ë§ì´', 'ì•½ê°„', 'ì ê¹', 'ë‹¤ì‹œ', 'ì•„ì§',
            'ë²Œì¨', 'ì´ë¯¸', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ìš”ì¦˜', 'ìµœê·¼', 'í˜„ì¬', 'ê³¼ê±°', 'ë¯¸ë˜'
        }
        
        # ë¬¸ë‹¨ ë¶„ë¦¬ íŒ¨í„´
        self.paragraph_patterns = [
            r'í•˜ì§€ë§Œ',
            r'ê·¸ëŸ¬ë‚˜', 
            r'ë”°ë¼ì„œ',
            r'ì´ì— ë”°ë¼',
            r'í•œí¸',
            r'ë˜í•œ',
            r'ê·¸ëŸ°ë°',
            r'ì´ì™€ í•¨ê»˜',
            r'ì´ë¥¼ í†µí•´',
            r'ì´ë²ˆì—',
            r'ìµœê·¼',
            r'í˜„ì¬',
            r'ê³¼ê±°',
            r'ë¯¸ë˜'
        ]
        
        # í‚¤ì›Œë“œ ë§¤í•‘ ì‚¬ì „
        self.keyword_mapping = {
            # ê²½ì œ/ê¸ˆìœµ
            'economy': 'ê²½ì œ', 'finance': 'ê¸ˆìœµ', 'investment': 'íˆ¬ì', 'stock': 'ì£¼ì‹',
            'market': 'ì‹œì¥', 'bank': 'ì€í–‰', 'trading': 'ê±°ë˜', 'fund': 'í€ë“œ',
            'asset': 'ìì‚°', 'profit': 'ì´ìµ', 'revenue': 'ìˆ˜ìµ', 'cost': 'ë¹„ìš©',
            'price': 'ê°€ê²©', 'value': 'ê°€ì¹˜', 'growth': 'ì„±ì¥', 'decline': 'í•˜ë½',
            
            # ê¸°ìˆ /IT
            'technology': 'ê¸°ìˆ ', 'ai': 'ì¸ê³µì§€ëŠ¥', 'artificial intelligence': 'ì¸ê³µì§€ëŠ¥',
            'machine learning': 'ë¨¸ì‹ ëŸ¬ë‹', 'blockchain': 'ë¸”ë¡ì²´ì¸', 'cryptocurrency': 'ì•”í˜¸í™”í',
            'bitcoin': 'ë¹„íŠ¸ì½”ì¸', 'ethereum': 'ì´ë”ë¦¬ì›€', 'software': 'ì†Œí”„íŠ¸ì›¨ì–´',
            'hardware': 'í•˜ë“œì›¨ì–´', 'digital': 'ë””ì§€í„¸', 'cloud': 'í´ë¼ìš°ë“œ',
            'internet': 'ì¸í„°ë„·', 'online': 'ì˜¨ë¼ì¸', 'mobile': 'ëª¨ë°”ì¼',
            
            # ì •ì¹˜/ì •ì±…
            'policy': 'ì •ì±…', 'government': 'ì •ë¶€', 'politics': 'ì •ì¹˜', 'law': 'ë²•ë¥ ',
            'regulation': 'ê·œì œ', 'bill': 'ë²•ì•ˆ', 'congress': 'ì˜íšŒ', 'senate': 'ìƒì›',
            'house': 'í•˜ì›', 'president': 'ëŒ€í†µë ¹', 'minister': 'ì¥ê´€', 'election': 'ì„ ê±°',
            
            # ë¹„ì¦ˆë‹ˆìŠ¤
            'business': 'ë¹„ì¦ˆë‹ˆìŠ¤', 'company': 'íšŒì‚¬', 'corporation': 'ê¸°ì—…', 'startup': 'ìŠ¤íƒ€íŠ¸ì—…',
            'ceo': 'ìµœê³ ê²½ì˜ì', 'management': 'ê²½ì˜ì§„', 'merger': 'í•©ë³‘', 'acquisition': 'ì¸ìˆ˜',
            'partnership': 'íŒŒíŠ¸ë„ˆì‹­', 'contract': 'ê³„ì•½', 'deal': 'ê±°ë˜', 'agreement': 'í˜‘ì •',
            
            # ì—ë„ˆì§€/í™˜ê²½
            'energy': 'ì—ë„ˆì§€', 'oil': 'ì„ìœ ', 'gas': 'ê°€ìŠ¤', 'renewable': 'ì¬ìƒì—ë„ˆì§€',
            'environment': 'í™˜ê²½', 'climate': 'ê¸°í›„', 'carbon': 'íƒ„ì†Œ', 'emission': 'ë°°ì¶œ',
            'green': 'ì¹œí™˜ê²½', 'sustainability': 'ì§€ì†ê°€ëŠ¥ì„±'
        }
    
    def extract_keywords(self, content: str) -> str:
        """
        ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            content: ë¶„ì„í•  ë‚´ìš©
            
        Returns:
            í•´ì‹œíƒœê·¸ í˜•ì‹ì˜ í‚¤ì›Œë“œ
        """
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        content = content.lower()
        content = re.sub(r'[^\w\sê°€-í£]', ' ', content)
        
        # ë‹¨ì–´ ë¶„ë¦¬
        words = content.split()
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        filtered_words = [
            word for word in words 
            if word not in self.stopwords and len(word) > 1
        ]
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(filtered_words)
        
        # ì˜ì–´ í‚¤ì›Œë“œ í•œêµ­ì–´ ë³€í™˜
        korean_keywords = []
        for word, freq in word_freq.items():
            if word in self.keyword_mapping:
                korean_keywords.append(self.keyword_mapping[word])
            elif freq > 1:  # 2íšŒ ì´ìƒ ì–¸ê¸‰ëœ ë‹¨ì–´ë§Œ
                korean_keywords.append(word)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        unique_keywords = list(dict.fromkeys(korean_keywords))[:7]
        
        # ì£¼ì‹ ì‹¬ë³¼ ê°ì§€
        stock_symbols = re.findall(r'\$[A-Z]{1,5}', content.upper())
        if stock_symbols:
            unique_keywords.extend([symbol.replace('$', '') for symbol in stock_symbols[:2]])
        
        # í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        hashtags = [f"#{keyword}" for keyword in unique_keywords[:7]]
        
        return ' '.join(hashtags)
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        
        Args:
            text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?ã€‚]+', text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _calculate_sentence_score(self, sentence: str, word_freq: Counter) -> float:
        """
        ë¬¸ì¥ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            sentence: ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë¬¸ì¥
            word_freq: ë‹¨ì–´ ë¹ˆë„ ì¹´ìš´í„°
            
        Returns:
            ë¬¸ì¥ ì ìˆ˜
        """
        words = sentence.lower().split()
        score = 0
        
        for word in words:
            if word in word_freq:
                score += word_freq[word]
        
        # ë¬¸ì¥ ê¸¸ì´ë¡œ ì •ê·œí™”
        if len(words) > 0:
            score = score / len(words)
        
        return score
    
    def _extract_key_sentences(self, content: str, num_sentences: int = 5) -> List[str]:
        """
        í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        
        Args:
            content: ë¶„ì„í•  ë‚´ìš©
            num_sentences: ì¶”ì¶œí•  ë¬¸ì¥ ìˆ˜
            
        Returns:
            í•µì‹¬ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        sentences = self._split_into_sentences(content)
        
        if len(sentences) <= num_sentences:
            return sentences
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        all_words = content.lower().split()
        word_freq = Counter([
            word for word in all_words 
            if word not in self.stopwords and len(word) > 1
        ])
        
        # ê° ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
        sentence_scores = []
        for i, sentence in enumerate(sentences):
            score = self._calculate_sentence_score(sentence, word_freq)
            sentence_scores.append((score, i, sentence))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì¥ ì„ íƒ
        sentence_scores.sort(key=lambda x: x[0], reverse=True)
        
        # ìƒìœ„ ë¬¸ì¥ë“¤ì„ ì›ë³¸ ìˆœì„œë¡œ ì¬ì •ë ¬
        selected_sentences = sentence_scores[:num_sentences]
        selected_sentences.sort(key=lambda x: x[1])
        
        return [sentence for _, _, sentence in selected_sentences]
    
    def _create_sections(self, content: str) -> List[Tuple[str, List[str]]]:
        """
        ë‚´ìš©ì„ ì„¹ì…˜ìœ¼ë¡œ êµ¬ë¶„
        
        Args:
            content: ë¶„ì„í•  ë‚´ìš©
            
        Returns:
            (ì„¹ì…˜ëª…, ë¬¸ì¥ë¦¬ìŠ¤íŠ¸) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        key_sentences = self._extract_key_sentences(content, 8)
        
        if len(key_sentences) <= 3:
            return [("ì£¼ìš” ë‚´ìš©", key_sentences)]
        
        # ë¬¸ì¥ë“¤ì„ ì£¼ì œë³„ë¡œ ê·¸ë£¹í™”
        sections = []
        current_section = []
        
        for i, sentence in enumerate(key_sentences):
            current_section.append(sentence)
            
            # 3-4ê°œ ë¬¸ì¥ë§ˆë‹¤ ìƒˆ ì„¹ì…˜ ìƒì„±
            if len(current_section) >= 3 and i < len(key_sentences) - 1:
                section_name = self._generate_section_name(current_section)
                sections.append((section_name, current_section.copy()))
                current_section = []
        
        # ë‚¨ì€ ë¬¸ì¥ë“¤ ì²˜ë¦¬
        if current_section:
            section_name = self._generate_section_name(current_section)
            sections.append((section_name, current_section))
        
        return sections
    
    def _generate_section_name(self, sentences: List[str]) -> str:
        """
        ë¬¸ì¥ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ëª… ìƒì„±
        
        Args:
            sentences: ì„¹ì…˜ì˜ ë¬¸ì¥ë“¤
            
        Returns:
            ì„¹ì…˜ëª…
        """
        # í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
        all_text = ' '.join(sentences).lower()
        words = all_text.split()
        
        # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter([
            word for word in words 
            if word not in self.stopwords and len(word) > 1
        ])
        
        if not word_freq:
            return "ì£¼ìš” ë‚´ìš©"
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ ê¸°ë°˜ ì„¹ì…˜ëª… ìƒì„±
        top_word = word_freq.most_common(1)[0][0]
        
        # ì„¹ì…˜ëª… ë§¤í•‘
        section_mapping = {
            'ì‹œì¥': 'ì‹œì¥ ë™í–¥',
            'ì£¼ì‹': 'ì£¼ì‹ í˜„í™©',
            'íˆ¬ì': 'íˆ¬ì ë™í–¥',
            'ê²½ì œ': 'ê²½ì œ ìƒí™©',
            'ì •ì±…': 'ì •ì±… ë³€í™”',
            'ê¸°ìˆ ': 'ê¸°ìˆ  ë°œì „',
            'íšŒì‚¬': 'ê¸°ì—… ë™í–¥',
            'ì„±ì¥': 'ì„±ì¥ ì „ë§',
            'ë³€í™”': 'ë³€í™” í˜„í™©',
            'ë°œí‘œ': 'ë°œí‘œ ë‚´ìš©',
            'ê³„íš': 'í–¥í›„ ê³„íš',
            'ê²°ê³¼': 'ì£¼ìš” ê²°ê³¼',
            'ì˜í–¥': 'ì˜í–¥ ë¶„ì„',
            'ì „ë§': 'ë¯¸ë˜ ì „ë§'
        }
        
        # ë§¤í•‘ì—ì„œ ì°¾ê¸°
        for key, value in section_mapping.items():
            if key in all_text:
                return value
        
        return "ì£¼ìš” ë‚´ìš©"
    
    def convert_to_markdown(self, data: Dict[str, str]) -> str:
        """
        ê·œì¹™ ê¸°ë°˜ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        
        Args:
            data: êµ¬ì¡°í™”ëœ ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´
        """
        # ë‚´ìš© ì •ì œ
        content = self.clean_content(data['content'])
        
        # í† í”½ ê°ì§€ ë° ì´ëª¨ì§€ ì„ íƒ
        topic = self.detect_topic(f"{data['title']} {data['description']} {content}")
        emoji = self.emoji_mapping.get(topic, 'ğŸ“°')
        
        # ì œëª© ìƒì„± (ì›ë³¸ ì œëª© í™œìš©)
        title = data['title'].strip()
        if not title:
            title = data['description'][:50] + "..."
        
        formatted_title = f"{emoji} {title}"
        
        # ì„¹ì…˜ ìƒì„±
        sections = self._create_sections(content)
        
        # ë§ˆí¬ë‹¤ìš´ êµ¬ì„±
        markdown_lines = [formatted_title, ""]
        
        for section_name, sentences in sections:
            markdown_lines.append(f"â–¶ {section_name}:")
            
            for sentence in sentences:
                # ë¬¸ì¥ ì •ë¦¬
                sentence = sentence.strip()
                if sentence:
                    # ì£¼ì‹ ì‹¬ë³¼ í¬ë§·íŒ…
                    sentence = self.format_stock_symbols(sentence)
                    markdown_lines.append(f"â€¢ {sentence}")
            
            markdown_lines.append("")  # ì„¹ì…˜ ê°„ ë¹ˆ ì¤„
        
        # ë§ˆì§€ë§‰ ë¹ˆ ì¤„ ì œê±°
        while markdown_lines and markdown_lines[-1] == "":
            markdown_lines.pop()
        
        return '\n'.join(markdown_lines) 