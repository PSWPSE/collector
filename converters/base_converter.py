"""
ë‰´ìŠ¤ ë³€í™˜ê¸° ë² ì´ìŠ¤ í´ë˜ìŠ¤

ëª¨ë“  ë³€í™˜ê¸°ì˜ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
"""

import os
import re
from datetime import datetime
from pathlib import Path
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
from dotenv import load_dotenv


class BaseConverter(ABC):
    """ë‰´ìŠ¤ ë³€í™˜ê¸° ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = 'data/generated'):
        """
        ë² ì´ìŠ¤ ë³€í™˜ê¸° ì´ˆê¸°í™”
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ê³µí†µ ì´ëª¨ì§€ ë§¤í•‘
        self.emoji_mapping = {
            'market': 'ğŸ“ˆ',
            'finance': 'ğŸ’°',
            'tech': 'ğŸš€',
            'policy': 'âš–ï¸',
            'trade': 'ğŸ¤',
            'energy': 'â›½',
            'conflict': 'ğŸ”¥',
            'growth': 'ğŸŒ±',
            'default': 'ğŸ“°'
        }
        
        # ì£¼ì‹ ì‹¬ë³¼ ë§¤í•‘
        self.company_symbols = {
            'Apple': 'AAPL',
            'Microsoft': 'MSFT',
            'Amazon': 'AMZN',
            'Google': 'GOOGL',
            'Tesla': 'TSLA',
            'Meta': 'META',
            'Netflix': 'NFLX',
            'Nvidia': 'NVDA',
            'Samsung': '005930',
            'SKí•˜ì´ë‹‰ìŠ¤': '000660',
            'LGì „ì': '066570'
        }
        
    def read_txt_file(self, file_path: str) -> Dict[str, str]:
        """
        TXT íŒŒì¼ì„ ì½ì–´ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë°˜í™˜
        
        Args:
            file_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            title, description, contentë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
        # í˜•ì‹ 1: ê¸°ì¡´ í˜•ì‹ (ì œëª©: xxx\n====)
        title_match = re.search(r'ì œëª©: (.*?)\n={2,}', content)
        meta_match = re.search(r'ë©”íƒ€ ì •ë³´:\ndescription: (.*?)\n-{2,}', content, re.DOTALL)
        content_match = re.search(r'ë³¸ë¬¸:\n(.*?)$', content, re.DOTALL)
        
        # í˜•ì‹ 2: ìƒˆë¡œìš´ í˜•ì‹ (URL:, ì œëª©:, ì €ì:, ë‚ ì§œ:, ì¶”ì¶œì¼:, ==ë¶„ë¦¬ì„ ==, ë³¸ë¬¸)
        if not title_match:
            # ì œëª© ì¶”ì¶œ (URL ë‹¤ìŒ ì¤„ë¶€í„° ì‹œì‘)
            url_match = re.search(r'URL: (.*?)\n', content)
            title_match_new = re.search(r'ì œëª©: (.*?)\n', content)
            author_match = re.search(r'ì €ì: (.*?)\n', content)
            date_match = re.search(r'ë‚ ì§œ: (.*?)\n', content)
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ë¶„ë¦¬ì„  ë‹¤ìŒë¶€í„° ëê¹Œì§€)
            content_match_new = re.search(r'={10,}\n\n(.*?)$', content, re.DOTALL)
            
            # ì‹¤ì œ ì œëª©ì´ ë³¸ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸ (Yahoo Finance ê°™ì€ ê²½ìš°)
            if content_match_new:
                content_text = content_match_new.group(1).strip()
                lines = content_text.split('\n')
                # ì²« ë²ˆì§¸ ì¤„ì—ì„œ ì‹¤ì œ ì œëª© ì°¾ê¸°
                real_title = ""
                for line in lines[:5]:  # ì²˜ìŒ 5ì¤„ë§Œ í™•ì¸
                    line = line.strip()
                    if line and len(line) > 20 and not any(skip in line.lower() for skip in ['sarah e.', 'fri,', 'min read', 'by taboola']):
                        real_title = line
                        break
                
                return {
                    'title': real_title if real_title else (title_match_new.group(1) if title_match_new else ""),
                    'description': f"ì €ì: {author_match.group(1) if author_match else ''}, ë‚ ì§œ: {date_match.group(1) if date_match else ''}",
                    'content': content_text
                }
        
        # ê¸°ë³¸ í˜•ì‹ ë°˜í™˜
        return {
            'title': title_match.group(1) if title_match else "",
            'description': meta_match.group(1).strip() if meta_match else "",
            'content': content_match.group(1).strip() if content_match else ""
        }
    
    def clean_content(self, content: str) -> str:
        """
        ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
        
        Args:
            content: ì›ë³¸ ë‚´ìš©
            
        Returns:
            ì •ë¦¬ëœ ë‚´ìš©
        """
        # ê¸°ì ì •ë³´ ì œê±°
        content = re.sub(r'By\s+[\w\s]+\n', '', content)
        content = re.sub(r'Kevin Buckland.*?\n', '', content, flags=re.IGNORECASE)
        content = re.sub(r'\w+@\w+\.\w+', '', content)  # ì´ë©”ì¼ ì œê±°
        
        # ë©”íƒ€ë°ì´í„° ì œê±°
        unwanted_patterns = [
            'usd=x', '^spx', 'terms and privacy', 'privacy dashboard',
            'fri,', 'min read', 'in this article', 'reporting by', 'editing by',
            'subscribe', 'newsletter', 'follow us', 'download app'
        ]
        
        lines = content.split('\n')
        filtered_lines = []
        
        for line in lines:
            line = line.strip()
            if (line and len(line) > 20 and 
                not any(pattern in line.lower() for pattern in unwanted_patterns)):
                filtered_lines.append(line)
        
        # ì¤‘ë³µ ì œê±°
        unique_lines = list(dict.fromkeys(filtered_lines))
        
        return '\n'.join(unique_lines)
    
    def detect_topic(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í† í”½ ê°ì§€
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ê°ì§€ëœ í† í”½
        """
        text_lower = text.lower()
        
        topic_keywords = {
            'market': ['market', 'stock', 'trading', 'index', 'equity', 'shares', 'ì£¼ì‹', 'ì¦ì‹œ', 'ì‹œì¥'],
            'finance': ['bank', 'finance', 'investment', 'fund', 'money', 'dollar', 'ì€í–‰', 'íˆ¬ì', 'ìê¸ˆ'],
            'tech': ['technology', 'tech', 'ai', 'digital', 'software', 'innovation', 'ê¸°ìˆ ', 'í˜ì‹ ', 'ì†Œí”„íŠ¸ì›¨ì–´'],
            'policy': ['policy', 'regulation', 'law', 'government', 'bill', 'tax', 'ì •ì±…', 'ê·œì œ', 'ë²•ë¥ '],
            'trade': ['trade', 'tariff', 'export', 'import', 'deal', 'agreement', 'ë¬´ì—­', 'ìˆ˜ì¶œ', 'ìˆ˜ì…'],
            'energy': ['oil', 'gas', 'energy', 'fuel', 'crude', 'petroleum', 'ì—ë„ˆì§€', 'ì„ìœ ', 'ê°€ìŠ¤']
        }
        
        scores = {topic: 0 for topic in topic_keywords.keys()}
        
        for topic, keywords in topic_keywords.items():
            for keyword in keywords:
                scores[topic] += text_lower.count(keyword)
        
        if not any(scores.values()):
            return 'default'
        
        return max(scores.items(), key=lambda x: x[1])[0]
    
    def format_stock_symbols(self, text: str) -> str:
        """
        ì£¼ì‹ ì‹¬ë³¼ í¬ë§·íŒ… ($SYMBOL í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
        """
        formatted = text
        for company, symbol in self.company_symbols.items():
            if company in formatted:
                formatted = formatted.replace(company, f"{company} ${symbol}")
        
        return formatted
    
    def generate_output_filename(self, input_path: str, suffix: str = "") -> Path:
        """
        ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        
        Args:
            input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            suffix: íŒŒì¼ëª… ì ‘ë¯¸ì‚¬
            
        Returns:
            ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        input_name = Path(input_path).stem
        
        if suffix:
            filename = f"{input_name}_{suffix}_{timestamp}.md"
        else:
            filename = f"{input_name}_{timestamp}.md"
        
        return self.output_dir / filename
    
    def save_markdown(self, content: str, output_path: Path) -> None:
        """
        ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        
        Args:
            content: ì €ì¥í•  ë‚´ìš©
            output_path: ì €ì¥ ê²½ë¡œ
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… Created: {output_path}")
    
    @abstractmethod
    def convert_to_markdown(self, data: Dict[str, str]) -> str:
        """
        ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (ê° ë³€í™˜ê¸°ì—ì„œ êµ¬í˜„)
        
        Args:
            data: êµ¬ì¡°í™”ëœ ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´
        """
        pass
    
    @abstractmethod
    def extract_keywords(self, content: str) -> str:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ (ê° ë³€í™˜ê¸°ì—ì„œ êµ¬í˜„)
        
        Args:
            content: ë¶„ì„í•  ë‚´ìš©
            
        Returns:
            í•´ì‹œíƒœê·¸ í˜•ì‹ì˜ í‚¤ì›Œë“œ
        """
        pass
    
    def process_file(self, file_path: str) -> None:
        """
        íŒŒì¼ ì²˜ë¦¬ (ê³µí†µ ì›Œí¬í”Œë¡œìš°)
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
        """
        print(f"ğŸ”„ Processing: {file_path}")
        
        try:
            # 1. íŒŒì¼ ì½ê¸°
            data = self.read_txt_file(file_path)
            
            # 2. ë§ˆí¬ë‹¤ìš´ ë³€í™˜
            markdown_content = self.convert_to_markdown(data)
            
            # 3. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(
                f"{data['title']}\n{data['description']}\n{data['content']}"
            )
            
            # 4. ìµœì¢… ì¡°í•©
            final_content = f"{markdown_content}\n\n{keywords}"
            
            # 5. íŒŒì¼ ì €ì¥
            output_path = self.generate_output_filename(
                file_path, 
                suffix=self.__class__.__name__.lower().replace('converter', '')
            )
            self.save_markdown(final_content, output_path)
            
        except Exception as e:
            print(f"âŒ Error processing {file_path}: {str(e)}")
    
    def process_directory(self, directory_path: str) -> None:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  TXT íŒŒì¼ ì²˜ë¦¬
        
        Args:
            directory_path: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        directory = Path(directory_path)
        txt_files = list(directory.glob('*.txt'))
        
        if not txt_files:
            print(f"âŒ No TXT files found in {directory_path}")
            return
        
        print(f"ğŸ“ Processing {len(txt_files)} files in {directory_path}")
        
        for txt_file in txt_files:
            self.process_file(str(txt_file)) 