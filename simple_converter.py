import os
from datetime import datetime
from pathlib import Path
import re
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

class SimpleNewsConverter:
    def __init__(self):
        self.output_dir = Path('converted_articles')
        self.output_dir.mkdir(exist_ok=True)
        
        # Download required NLTK data
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('taggers/averaged_perceptron_tagger')
            nltk.data.find('corpora/stopwords')
            nltk.data.find('tokenizers/punkt_tab/english')
        except LookupError:
            nltk.download('punkt')
            nltk.download('averaged_perceptron_tagger')
            nltk.download('stopwords')
            nltk.download('punkt_tab')

        # ì´ëª¨ì§€ ë§¤í•‘ ì •ì˜
        self.emoji_mapping = {
            'stock': 'ğŸ“ˆ',
            'market': 'ğŸ“Š',
            'finance': 'ğŸ’°',
            'tech': 'ğŸš€',
            'policy': 'âš–ï¸',
            'conflict': 'ğŸ”¥',
            'growth': 'ğŸŒ±',
            'deal': 'ğŸ¤',
            'default': 'ğŸ“°'
        }

        # ì£¼ìš” í‚¤ì›Œë“œ ë§¤í•‘
        self.topic_keywords = {
            'stock': ['stock', 'shares', 'market', 'trading', 'nasdaq', 'dow', 's&p'],
            'market': ['market', 'investors', 'trading', 'economy', 'wall street'],
            'finance': ['bank', 'finance', 'investment', 'fund', 'money'],
            'tech': ['technology', 'tech', 'ai', 'digital', 'software', 'innovation'],
            'policy': ['policy', 'regulation', 'law', 'government', 'bill', 'tax'],
            'conflict': ['dispute', 'lawsuit', 'controversy', 'battle', 'fight'],
            'growth': ['growth', 'expansion', 'development', 'increase'],
            'deal': ['deal', 'agreement', 'partnership', 'acquisition', 'merger']
        }

    def read_txt_file(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # Parse the content
        title_match = re.search(r'ì œëª©: (.*?)\n={2,}', content)
        meta_match = re.search(r'ë©”íƒ€ ì •ë³´:\ndescription: (.*?)\n-{2,}', content, re.DOTALL)
        content_match = re.search(r'ë³¸ë¬¸:\n(.*?)$', content, re.DOTALL)
        
        title = title_match.group(1) if title_match else ""
        description = meta_match.group(1).strip() if meta_match else ""
        main_content = content_match.group(1).strip() if content_match else ""
        
        return {
            'title': title,
            'description': description,
            'content': main_content
        }

    def detect_topic(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í† í”½ì„ ê°ì§€í•˜ì—¬ ì ì ˆí•œ ì´ëª¨ì§€ ì„ íƒ"""
        text = text.lower()
        scores = {topic: 0 for topic in self.topic_keywords.keys()}
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    scores[topic] += 1
        
        if not any(scores.values()):
            return 'default'
        
        return max(scores.items(), key=lambda x: x[1])[0]

    def extract_stock_symbols(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì‹ ì‹¬ë³¼ ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì£¼ì‹ ì‹¬ë³¼ íŒ¨í„´ ($AAPL, $TSLA ë“±)
        symbols = re.findall(r'\$([A-Z]{1,5})', text)
        
        # íšŒì‚¬ ì´ë¦„ ë’¤ì— ì‹¬ë³¼ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        companies = {
            'Apple': 'AAPL',
            'Microsoft': 'MSFT',
            'Amazon': 'AMZN',
            'Google': 'GOOGL',
            'Tesla': 'TSLA',
            'Meta': 'META',
            'Netflix': 'NFLX',
            'Nvidia': 'NVDA'
        }
        
        for company, symbol in companies.items():
            if company in text and symbol not in symbols:
                symbols.append(symbol)
        
        return list(set(symbols))

    def extract_keywords(self, text, num_keywords=5):
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        words = word_tokenize(text.lower())
        stop_words = set(stopwords.words('english'))
        
        # ë¶ˆìš©ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        words = [word for word in words if word.isalnum() and word not in stop_words and len(word) > 2]
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter(words)
        keywords = [word for word, _ in word_freq.most_common(num_keywords)]
        
        return keywords

    def structure_content(self, content):
        """ë‚´ìš©ì„ ì„¹ì…˜ë³„ë¡œ êµ¬ì¡°í™”"""
        paragraphs = content.split('\n\n')
        
        # ì„¹ì…˜ êµ¬ë¶„
        sections = {
            'â–¶ ì£¼ìš” ë‚´ìš©:': [],
            'â–¶ ì˜í–¥ ë° ì „ë§:': [],
            'â–¶ ê´€ë ¨ ë™í–¥:': []
        }
        
        # ë¬¸ë‹¨ ë¶„ë¥˜
        for i, para in enumerate(paragraphs):
            if i == 0:
                sections['â–¶ ì£¼ìš” ë‚´ìš©:'].append(para)
            elif 'ì˜í–¥' in para.lower() or 'ì „ë§' in para.lower() or 'ì˜ˆìƒ' in para.lower():
                sections['â–¶ ì˜í–¥ ë° ì „ë§:'].append(para)
            else:
                sections['â–¶ ê´€ë ¨ ë™í–¥:'].append(para)
        
        # ì„¹ì…˜ í…ìŠ¤íŠ¸ ìƒì„±
        structured_content = []
        for section, paras in sections.items():
            if paras:
                structured_content.append(f"\n{section}")
                for para in paras:
                    structured_content.append(f"â€¢ {para.strip()}")
        
        return '\n'.join(structured_content)

    def format_stock_mentions(self, text, symbols):
        """ì£¼ì‹ ì‹¬ë³¼ í¬ë§·íŒ…"""
        formatted = text
        for symbol in symbols:
            # íšŒì‚¬ ì´ë¦„ ì°¾ì•„ì„œ ì‹¬ë³¼ ì¶”ê°€
            companies = {
                'Apple': 'AAPL',
                'Microsoft': 'MSFT',
                'Amazon': 'AMZN',
                'Google': 'GOOGL',
                'Tesla': 'TSLA',
                'Meta': 'META',
                'Netflix': 'NFLX',
                'Nvidia': 'NVDA'
            }
            
            for company, sym in companies.items():
                if sym == symbol and company in formatted:
                    formatted = formatted.replace(
                        company,
                        f"{company} ${symbol}"
                    )
        
        return formatted

    def convert_to_markdown(self, data):
        """ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # í† í”½ ê°ì§€ ë° ì´ëª¨ì§€ ì„ íƒ
        topic = self.detect_topic(data['title'] + ' ' + data['description'])
        emoji = self.emoji_mapping.get(topic, self.emoji_mapping['default'])
        
        # ì£¼ì‹ ì‹¬ë³¼ ì¶”ì¶œ
        symbols = self.extract_stock_symbols(data['title'] + ' ' + data['content'])
        
        # ì œëª© í¬ë§·íŒ…
        title = self.format_stock_mentions(data['title'], symbols)
        formatted_title = f"{emoji} {title}"
        
        # ë‚´ìš© êµ¬ì¡°í™” ë° í¬ë§·íŒ…
        content = self.format_stock_mentions(data['content'], symbols)
        structured_content = self.structure_content(content)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(
            data['title'] + ' ' + data['description'] + ' ' + data['content']
        )
        keyword_tags = ' '.join([f"#{keyword}" for keyword in keywords])
        
        # ìµœì¢… ë§ˆí¬ë‹¤ìš´ ìƒì„±
        markdown = f"{formatted_title}\n\n{structured_content}\n\n{keyword_tags}"
        
        return markdown

    def process_file(self, file_path):
        """ë‹¨ì¼ TXT íŒŒì¼ ì²˜ë¦¬"""
        print(f"Processing {file_path}...")
        data = self.read_txt_file(file_path)
        markdown_content = self.convert_to_markdown(data)
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f"{Path(file_path).stem}_{timestamp}.md"
        output_path = self.output_dir / output_filename
        
        # ë§ˆí¬ë‹¤ìš´ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"Created {output_path}")

    def process_directory(self, directory_path):
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  TXT íŒŒì¼ ì²˜ë¦¬"""
        directory = Path(directory_path)
        for txt_file in directory.glob('*.txt'):
            self.process_file(txt_file)

def main():
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python simple_converter.py <txt_file_or_directory>")
        sys.exit(1)
    
    path = sys.argv[1]
    converter = SimpleNewsConverter()
    
    if os.path.isfile(path):
        converter.process_file(path)
    elif os.path.isdir(path):
        converter.process_directory(path)
    else:
        print(f"Error: {path} is not a valid file or directory")
        sys.exit(1)

if __name__ == '__main__':
    main() 